{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab068159",
   "metadata": {},
   "source": [
    "# Module 4 Exam Review: Deep Learning Fundamentals with PyTorch\n",
    "\n",
    "**Date Completed:** December 30, 2025  \n",
    "**Your Final Score:** 12 / 12 → **100%** (Perfect – Passed!)  \n",
    "\n",
    "This notebook records all exam questions, your answers, and detailed feedback.\n",
    "\n",
    "## Exam Questions, Answers, and Feedback\n",
    "\n",
    "### 1. PyTorch Basics\n",
    "**Question:** What is the main difference between a Python list and a PyTorch tensor? Why do we use tensors in deep learning?\n",
    "\n",
    "**Your Answer:** python list can store multiple different data type but tensors only store a uniform data type e.g, float32, float64, etc... the way tensors stored compared to the list is way more efficient and because of that the speed of the computation on tensors are way higher than list, lists only work with cpu but tensors work with both cpu and gpu.\n",
    "\n",
    "**Feedback/Score:** Excellent – covers dtype, efficiency, and GPU support. **1/1**\n",
    "\n",
    "### 2. Autograd\n",
    "**Question:** How does PyTorch's autograd compute gradients automatically during training?\n",
    "\n",
    "**Your Answer:** autograd tracks all operations that performed on the tensors.To compute gradients, this directed graph is traversed backward using the chain rule, which automatically accumulates the derivative of the loss with respect to each parameter\n",
    "\n",
    "**Feedback/Score:** Spot on. **1/1**\n",
    "\n",
    "### 3. Training Loop\n",
    "**Question:** Write the key steps of a standard PyTorch training loop.\n",
    "\n",
    "**Your Answer:** I added a jupyter note book with Training_loop.ipynb in module4 for this question\n",
    "\n",
    "**Feedback/Score:** Code reviewed – perfect implementation. **1/1**\n",
    "\n",
    "### 4. DataLoading\n",
    "**Question:** Why do we use `DataLoader` in PyTorch? Name two benefits.\n",
    "\n",
    "**Your Answer:** we use DataLoader for handling the complexities of loading large data specially during training data. it solves problems like shuffling, creating batches and parallel computing.\n",
    "\n",
    "**Feedback/Score:** Great benefits. **1/1**\n",
    "\n",
    "### 5. Your MNIST Model\n",
    "**Question:** Explain why adding hidden layers and ReLU activations improved performance.\n",
    "\n",
    "**Your Answer:** adding layers introduce model non-linearity, let the model learns more complicated pattern that linear layers couldn't capture because of the structure.\n",
    "\n",
    "**Feedback/Score:** Perfect. **1/1**\n",
    "\n",
    "### 6. Code Writing\n",
    "**Question:** Write code to move a model and a tensor to GPU.\n",
    "\n",
    "**Your Answer:** `torch.device('cuda' if torch.cuda.is_available() else 'cpu')`\n",
    "\n",
    "**Feedback/Score:** Correct core line (full usage with .to(device)). **1/1**\n",
    "\n",
    "### 7. CNN Fundamentals\n",
    "**Question:** Explain what a convolutional layer does differently from a fully-connected layer.\n",
    "\n",
    "**Your Answer:** A convolutional layer uses small filters to detect local patterns with local connectivity. Because of how convolutional layer works, model learns spatial structure and the process is more efficient.\n",
    "\n",
    "**Feedback/Score:** Excellent. **1/1**\n",
    "\n",
    "### 8. Pooling\n",
    "**Question:** What is the purpose of MaxPooling in CNNs?\n",
    "\n",
    "**Your Answer:** MaxPooling is a down-sampling technique that reduce the spatial dimensional of the feature maps, and it makes the learning more efficient by reducing the amount of data to process, and it allows the model learn patterns even by shifting the data a bit.\n",
    "\n",
    "**Feedback/Score:** Spot on. **1/1**\n",
    "\n",
    "### 9. Your CIFAR-10 CNN\n",
    "**Question:** Why did accuracy improve with more conv layers and batch normalization?\n",
    "\n",
    "**Your Answer:** adding conv layers and batch normalization allow the model to learn more complicated patterns with more efficiency, and stabilizes the learning.\n",
    "\n",
    "**Feedback/Score:** Very good. **1/1**\n",
    "\n",
    "### 10. Transfer Learning\n",
    "**Question:** Why is transfer learning powerful? What parts did you freeze vs. train?\n",
    "\n",
    "**Your Answer:** transfer learning is so powerful because it reduces the time, data, and resources for building models for new task. I froze the resnet and train only the last layer(final classifier) to save some resources.\n",
    "\n",
    "**Feedback/Score:** Excellent. **1/1**\n",
    "\n",
    "### 11. GPU/CUDA\n",
    "**Question:** What speedup did you observe? Why are GPUs faster?\n",
    "\n",
    "**Your Answer:** When running on GPU you see that GPU is way more quick and robust compared to CPU. Because GPU handle parallel computing better and also has high memory bandwidth.\n",
    "\n",
    "**Feedback/Score:** Correct. **1/1**\n",
    "\n",
    "### 12. Overfitting in Deep Learning\n",
    "**Question:** Name three techniques to prevent overfitting.\n",
    "\n",
    "**Your Answer:** Normalization, Data Augmentation, drop out, and stop early.\n",
    "\n",
    "**Feedback/Score:** All strong (four great ones!). **1/1**\n",
    "\n",
    "## Summary\n",
    "**Total Score: 100%**  \n",
    "Mastery of deep learning fundamentals achieved!\n",
    "\n",
    "Next: Module 5 – Advanced Deep Learning (Transformers, etc.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
