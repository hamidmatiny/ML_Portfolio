{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzNUf5F4WpW-"
      },
      "source": [
        "# 04. PyTorch Custom Datasets Notebook\n",
        "\n",
        "We've used some datasets with PyTorch before.\n",
        "\n",
        "But how do you get your own data into PyTorch?\n",
        "\n",
        "One of the ways to do so is via: custom datasets.\n",
        "\n",
        "## Domain libraries\n",
        "\n",
        "Depending on what you're working on, vision, text, audio, recommendation, you'll want to look into each of the PyTorch domain libraries for existing data loading functions and customizable data loading functions.\n",
        "\n",
        "**Resources:**\n",
        "* Book version of the course materials for 04: https://www.learnpytorch.io/04_pytorch_custom_datasets/\n",
        "* Ground truth version of notebook 04: https://github.com/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5kRBiQ1W51N"
      },
      "source": [
        "## 0. Importing PyTorch and setting up device-agnostic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vyrYSNVrXHhq",
        "outputId": "72b1079f-92b6-4942-fdea-c4471636df2e"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "\n",
        "# Note: PyTorch 1.10.0+ is required for this course\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FkRwIJurXh-R",
        "outputId": "068e0e27-fc11-46f5-833e-43c29ac12c40"
      },
      "outputs": [],
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ga8_C2kXsyB",
        "outputId": "64f12959-b232-45c5-83f7-3942889e74ee"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBL2AEj-YVYm"
      },
      "source": [
        "## 1. Get data \n",
        "\n",
        "Our dataset is a subset of the Food101 dataset.\n",
        "\n",
        "Food101 starts 101 different classes of food and 1000 images per class (750 training, 250 testing).\n",
        "\n",
        "Our dataset starts with 3 classes of food and only 10% of the images (~75 training, 25 testing).\n",
        "\n",
        "Why do this?\n",
        "\n",
        "When starting out ML projects, it's important to try things on a small scale and then increase the scale when necessary.\n",
        "\n",
        "The whole point is to speed up how fast you can experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eai10FjYdpe",
        "outputId": "1f468e8b-6306-4f85-b394-829fe92945f0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to a data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory already exists... skipping download\")\n",
        "else: \n",
        "  print(f\"{image_path} does not exist, creating one...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak and suhsi data\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloading pizza, steak, suhsi data...\")\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping pizza, steak and sushi data...\")\n",
        "  zip_ref.extractall(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj78f6eOblOw"
      },
      "source": [
        "## 2. Becoming one with the data (data preparation and data exploration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blF-1P-nc-Vs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"Walks through dir_path returning its contents.\"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrh1at6cdl0A",
        "outputId": "911296bf-1471-45f7-8ba2-77fdb2f8b3b1"
      },
      "outputs": [],
      "source": [
        "walk_through_dir(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdD5a16sdoiX",
        "outputId": "d4fa3822-a426-4bd8-95df-3c749194c994"
      },
      "outputs": [],
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hPLW9-FeF5S"
      },
      "source": [
        "### 2.1 Visualizing and image\n",
        "\n",
        "Let's write some code to:\n",
        "1. Get all of the image paths \n",
        "2. Pick a random image path using Python's random.choice()\n",
        "3. Get the image class name using `pathlib.Path.parent.stem` \n",
        "4. Since we're working with images, let's open the image with Python's PIL\n",
        "5. We'll then show the image and print metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIzz0wB5f7_8",
        "outputId": "3c3d8893-8259-411a-e6ae-eaa6960dc47c"
      },
      "outputs": [],
      "source": [
        "image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRye-sB0f_FB"
      },
      "outputs": [],
      "source": [
        "# /content/data/pizza_steak_sushi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "bZTEcDPzfNz1",
        "outputId": "1744c96d-dc4a-4be6-9b6a-600cde7afb7d"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "from PIL import Image\n",
        "\n",
        "# Set seed\n",
        "# random.seed(42)\n",
        "\n",
        "# 1. Get all image paths \n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "\n",
        "# 2. Pick a random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
        "image_class = random_image_path.parent.stem\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata \n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "NJG4KnZ5gXMt",
        "outputId": "717352f2-57b4-464c-fa1a-5c83384f2593"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Turn the image into an array\n",
        "img_as_array = np.asarray(img)\n",
        "\n",
        "# Plot the image with matplotlib\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(img_as_array)\n",
        "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels] (HWC)\")\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-fuYkffimxq",
        "outputId": "16667fd7-76a2-471d-c40f-cf0d28c531e9"
      },
      "outputs": [],
      "source": [
        "img_as_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YskhOYHAi46g"
      },
      "source": [
        "## 3. Transforming data \n",
        "\n",
        "Before we can use our image data with PyTorch:\n",
        "1. Turn your target data into tensors (in our case, numerical representation of our images).\n",
        "2. Turn it into a `torch.utils.data.Dataset` and subsequently a `torch.utils.data.DataLoader`, we'll call these `Dataset` and `DataLoader`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6hZmzlsOn_f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmQsZnTtOvAF"
      },
      "source": [
        "### 3.1 Transforming data with `torchvision.transforms`\n",
        "\n",
        "Transforms help you get your images ready to be used with a model/perform data augmentation - https://pytorch.org/vision/stable/transforms.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlG9ONP3Oydh"
      },
      "outputs": [],
      "source": [
        "# Write a transform for image\n",
        "data_transform = transforms.Compose([\n",
        "  # Resize our images to 64x64\n",
        "  transforms.Resize(size=(64, 64)),\n",
        "  # Flip the images randomly on the horizontal\n",
        "  transforms.RandomHorizontalFlip(p=0.5),\n",
        "  # Turn the image into a torch.Tensor\n",
        "  transforms.ToTensor() \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar0JqO_ePmDu",
        "outputId": "aa93acf4-c374-4cb9-d47d-104b4f814537"
      },
      "outputs": [],
      "source": [
        "data_transform(img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "sNTjJhSkQLuu",
        "outputId": "cfe0f7c4-2bef-4d15-cb2a-e27c2ceebed2"
      },
      "outputs": [],
      "source": [
        "def plot_transformed_images(image_paths: list, transform, n=3, seed=None):\n",
        "  \"\"\"\n",
        "  Selects random images from a path of images and loads/transforms \n",
        "  them then plots the original vs the transformed version.\n",
        "  \"\"\"\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "  random_image_paths = random.sample(image_paths, k=n)\n",
        "  for image_path in random_image_paths:\n",
        "    with Image.open(image_path) as f:\n",
        "      fig, ax = plt.subplots(nrows=1, ncols=2)\n",
        "      ax[0].imshow(f)\n",
        "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
        "      ax[0].axis(False)\n",
        "\n",
        "      # Transform and plot target image\n",
        "      transformed_image = transform(f).permute(1, 2, 0) # note we will need to change shape for matplotlib (C, H, W) -> (H, W, C)\n",
        "      ax[1].imshow(transformed_image)\n",
        "      ax[1].set_title(f\"Transformed\\nShape: {transformed_image.shape}\")\n",
        "      ax[1].axis(\"off\")\n",
        "\n",
        "      fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
        "\n",
        "plot_transformed_images(image_paths=image_path_list,\n",
        "                        transform=data_transform,\n",
        "                        n=3,\n",
        "                        seed=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4tTpsKJQtZc"
      },
      "source": [
        "## 4. Option 1: Loading image data using `ImageFolder`\n",
        "\n",
        "We can load image classification data using `torchvision.datasets.ImageFolder` - https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a4xJMAvQuDl",
        "outputId": "747db3bc-ba87-48da-e02e-3af96b3aa34d"
      },
      "outputs": [],
      "source": [
        "# Use ImageFolder to create dataset(s)\n",
        "from torchvision import datasets\n",
        "train_data = datasets.ImageFolder(root=train_dir,\n",
        "                                  transform=data_transform, # a transform for the data (data augmentation)\n",
        "                                  target_transform=None) # a transform for the label/target \n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform=data_transform)\n",
        "\n",
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI0dBu8tUueA",
        "outputId": "9ab9a763-60f5-496f-f906-4d007f7cbc43"
      },
      "outputs": [],
      "source": [
        "train_dir, test_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfBVkmf0U0Y8",
        "outputId": "b2c837cd-ebfe-4334-ef2d-fa3e9dc3d726"
      },
      "outputs": [],
      "source": [
        "# Get class names as list\n",
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t45RjwEFVCjo",
        "outputId": "0f4ec9a3-c88d-428c-e6c7-86036c908d68"
      },
      "outputs": [],
      "source": [
        "# Get class names as dict\n",
        "class_dict = train_data.class_to_idx\n",
        "class_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TO139f9VL1Y",
        "outputId": "3f86176f-e4da-4484-b01a-d8140c5d7508"
      },
      "outputs": [],
      "source": [
        "# Check the lengths of our dataset\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twhgvusTVVW-",
        "outputId": "fb5cfcb8-f475-4c91-f1a1-9f7cd44dafef"
      },
      "outputs": [],
      "source": [
        "train_data.samples[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSyhT2NsVZpK",
        "outputId": "3fd200f6-eb19-4562-8f8b-641608d26339"
      },
      "outputs": [],
      "source": [
        "# Index on the train_data Dataset to get a single image and label\n",
        "img, label = train_data[0][0], train_data[0][1]\n",
        "print(f\"Image tensor:\\n {img}\")\n",
        "print(f\"Image shape: {img.shape}\")\n",
        "print(f\"Image datatype: {img.dtype}\")\n",
        "print(f\"Image label: {label}\")\n",
        "print(f\"Label datatype: {type(label)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "64Bsf7zUV9ly",
        "outputId": "9d5c9126-b120-4dea-825a-d8e78e19df02"
      },
      "outputs": [],
      "source": [
        "# Rearrange the order dimensions\n",
        "img_permute = img.permute(1, 2, 0)\n",
        "\n",
        "# Print out different shapes\n",
        "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Image permute: {img_permute.shape} -> [height, width, color_channels]\")\n",
        "\n",
        "# Plot the image\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(img_permute)\n",
        "plt.axis(\"off\")\n",
        "plt.title(class_names[label], fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypsqb6PNV9jK"
      },
      "source": [
        "### 4.1 Turn loaded images into `DataLoader`'s \n",
        "\n",
        "A `DataLoader` is going to help us turn our `Dataset`'s into iterables and we can customise the `batch_size` so our model can see `batch_size` images at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWuFG4d_Y2S-",
        "outputId": "a621ff12-552e-4062-8608-2021f6c188fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAqirTxTV9d5",
        "outputId": "02b3e9fd-750a-46d0-e76d-90cb93282201"
      },
      "outputs": [],
      "source": [
        "# Turn train and test datasets into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE=1\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              num_workers=16,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             num_workers=16,\n",
        "                             shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4YBZgUEV9YF",
        "outputId": "f05dab4a-c492-49d5-9e58-11980c81a73b"
      },
      "outputs": [],
      "source": [
        "len(train_dataloader), len(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Al2lrqZaeR",
        "outputId": "bfe45f8b-2e45-4471-8640-227ff97ae293"
      },
      "outputs": [],
      "source": [
        "img, label = next(iter(train_dataloader))\n",
        "\n",
        "# Batch size will now be 1, you can change the batch size if you like\n",
        "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3XUC_RQZv2h"
      },
      "source": [
        "## 5 Option 2: Loading Image Data with a Custom `Dataset`\n",
        "\n",
        "1. Want to be able to load images from file\n",
        "2. Want to be able to get class names from the Dataset\n",
        "3. Want to be able to get classes as dictionary from the Dataset\n",
        "\n",
        "Pros:\n",
        "* Can create a `Dataset` out of almost anything\n",
        "* Not limited to PyTorch pre-built `Dataset` functions\n",
        "\n",
        "Cons:\n",
        "* Even though you could create `Dataset` out of almost anything, it doesn't mean it will work...\n",
        "* Using a custom `Dataset` often results in us writing more code, which could be prone to errors or performance issues \n",
        "\n",
        "All custom datasets in PyTorch, often subclass - https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGcOGecpKLnf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, Dict, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYRDOCtvKmj3",
        "outputId": "78fb4d02-6672-461b-f0d3-c4e1f3261538"
      },
      "outputs": [],
      "source": [
        "# Instance of torchvision.datasets.ImageFolder()\n",
        "train_data.classes, train_data.class_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYDsBrorKsp8"
      },
      "source": [
        "### 5.1 Creating a helper function to get class names\n",
        "\n",
        "We want a function to:\n",
        "1. Get the class names using `os.scandir()` to traverse a target directory (ideally the directory is in standard image classification format).\n",
        "2. Raise an error if the class names aren't found (if this happens, there might be something wrong with the directory structure).\n",
        "3. Turn the class names into a dict and a list and return them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9xxzUxLyK_",
        "outputId": "2b4cba45-15cc-48d2-b990-8b4e27612344"
      },
      "outputs": [],
      "source": [
        "# Setup path for target directory\n",
        "target_directory = train_dir\n",
        "print(f\"Target dir: {target_directory}\")\n",
        "\n",
        "# Get the class names from the target directory \n",
        "class_names_found = sorted([entry.name for entry in list(os.scandir(target_directory))])\n",
        "class_names_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXzh6PWvL4VL",
        "outputId": "0f48b688-c627-4262-ac1d-5dc66b0552bd"
      },
      "outputs": [],
      "source": [
        "list(os.scandir(target_directory))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "834YXQuhMPpV"
      },
      "outputs": [],
      "source": [
        "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "  \"\"\"Finds the class folder names in a target directory.\"\"\"\n",
        "  # 1. Get the class names by scanning the target directory\n",
        "  classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
        "\n",
        "  # 2. Raise an error if class names could not be found\n",
        "  if not classes:\n",
        "    raise FileNotFoundError(f\"Couldn't find any classes in {directory}... please check file structure.\")\n",
        "\n",
        "  # 3. Create a dictionary of index labels (computers prefer numbers rather than strings as labels)\n",
        "  class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
        "  return classes, class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAtx_es1MzYO",
        "outputId": "92445bae-4bf8-4d82-b48f-a8fc58f840eb"
      },
      "outputs": [],
      "source": [
        "find_classes(target_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUUWNMf4M1IA"
      },
      "source": [
        "### 5.2 Create a custom `Dataset` to replicate `ImageFolder`\n",
        "\n",
        "To create our own custom dataset, we want to:\n",
        "\n",
        "1. Subclass `torch.utils.data.Dataset`\n",
        "2. Init our subclass with a target directory (the directory we'd like to get data from) as well as a transform if we'd like to transform our data.\n",
        "3. Create several attributes:\n",
        "  * paths - paths of our images\n",
        "  * transform - the transform we'd like to use \n",
        "  * classes - a list of the target classes\n",
        "  * class_to_idx - a dict of the target classes mapped to integer labels\n",
        "4. Create a function to `load_images()`, this function will open an image\n",
        "5. Overwrite the `__len()__` method to return the length of our dataset\n",
        "6. Overwrite the `__getitem()__` method to return a given sample when passed an index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9acmLzJPcG1"
      },
      "outputs": [],
      "source": [
        "# 0. Write a custom dataset class\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 1. Subclass torch.utils.data.Dataset\n",
        "class ImageFolderCustom(Dataset):\n",
        "  # 2. Initialize our custom dataset\n",
        "  def __init__(self, \n",
        "               targ_dir: str, \n",
        "               transform=None):\n",
        "    # 3. Create class attributes\n",
        "    # Get all of the image paths\n",
        "    self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\"))\n",
        "    # Setup transform\n",
        "    self.transform = transform\n",
        "    # Create classes and class_to_idx attributes\n",
        "    self.classes, self.class_to_idx = find_classes(targ_dir)\n",
        "\n",
        "  # 4. Create a function to load images\n",
        "  def load_image(self, index: int) -> Image.Image:\n",
        "    \"Opens an image via a path and returns it.\"\n",
        "    image_path = self.paths[index]\n",
        "    return Image.open(image_path)\n",
        "\n",
        "  # 5. Overwrite __len__()\n",
        "  def __len__(self) -> int:\n",
        "    \"Returns the total number of samples.\"\n",
        "    return len(self.paths)\n",
        "  \n",
        "  # 6. Overwrite __getitem__() method to return a particular sample\n",
        "  def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
        "    \"Returns one sample of data, data and label (X, y).\"\n",
        "    img = self.load_image(index)\n",
        "    class_name = self.paths[index].parent.name # expects path in format: data_folder/class_name/image.jpg\n",
        "    class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "    # Transform if necessary\n",
        "    if self.transform:\n",
        "      return self.transform(img), class_idx # return data, label (X, y)\n",
        "    else:\n",
        "      return img, class_idx # return untransformed image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NsakYTURS_Z"
      },
      "outputs": [],
      "source": [
        "img, label = train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whvz78hbRV2X",
        "outputId": "90540e20-0ab6-4231-cbc7-65a6f3897a44"
      },
      "outputs": [],
      "source": [
        "img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zk7qByQvkjy"
      },
      "outputs": [],
      "source": [
        "# Create a transform\n",
        "from torchvision import transforms\n",
        "train_transforms = transforms.Compose([\n",
        "                                      transforms.Resize(size=(64, 64)),\n",
        "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                      transforms.ToTensor() \n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                                      transforms.Resize(size=(64, 64)),\n",
        "                                      transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qunV3fsGwRx-"
      },
      "outputs": [],
      "source": [
        "# Test out ImageFolderCustom\n",
        "train_data_custom = ImageFolderCustom(targ_dir=train_dir,\n",
        "                                      transform=train_transforms)\n",
        "\n",
        "test_data_custom = ImageFolderCustom(targ_dir=test_dir,\n",
        "                                     transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7F47q_Wwi34",
        "outputId": "dbb2be74-66a1-41f1-e453-a53b7c93e416"
      },
      "outputs": [],
      "source": [
        "train_data_custom, test_data_custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RajEu8OIwlUs",
        "outputId": "67a9be0e-1147-4327-e825-2b2561e057c9"
      },
      "outputs": [],
      "source": [
        "len(train_data), len(train_data_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHo0sNkKwn3G",
        "outputId": "cc903b45-79ea-4fe5-f6af-925238ad1546"
      },
      "outputs": [],
      "source": [
        "len(test_data), len(test_data_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUzWeHN9wwLY",
        "outputId": "cf05d6cb-9dd5-40c6-ae12-404cd991b054"
      },
      "outputs": [],
      "source": [
        "train_data_custom.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO5vXi8gwzWN",
        "outputId": "6ca66276-531e-4a44-8b42-c02b294f5122"
      },
      "outputs": [],
      "source": [
        "train_data_custom.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83MZXc2bw8RL",
        "outputId": "0f37281b-4db7-4d96-9155-9072c148910a"
      },
      "outputs": [],
      "source": [
        "# Check for equality between original ImageFolder Dataset and ImageFolderCustom Dataset\n",
        "print(train_data_custom.classes==train_data.classes)\n",
        "print(test_data_custom.classes==test_data.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Uw5fvuxML-"
      },
      "source": [
        "### 5.3 Create a function to display random images\n",
        "\n",
        "1. Take in a `Dataset` and a number of other parameters such as class names and how many images to visualize.\n",
        "2. To prevent the display getting out of hand, let's cap the number of images to see at 10.\n",
        "3. Set the random seed for reproducibility\n",
        "4. Get a list of random sample indexes from the target dataset.\n",
        "5. Setup a matplotlib plot.\n",
        "6. Loop through the random sample indexes and plot them with matploltib.\n",
        "7. Make sure the dimensions of our images line up with matplotlib (HWC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiyWjliqyhbW"
      },
      "outputs": [],
      "source": [
        "# 1. Create a function to take in a dataset\n",
        "def display_random_images(dataset: torch.utils.data.Dataset,\n",
        "                          classes: List[str] = None,\n",
        "                          n: int = 10,\n",
        "                          display_shape: bool = True,\n",
        "                          seed: int = None):\n",
        "  # 2. Adjust display if n is too high\n",
        "  if n > 10:\n",
        "    n = 10\n",
        "    display_shape = False\n",
        "    print(f\"For display, purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
        "\n",
        "  # 3. Set the seed\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "\n",
        "  # 4. Get random sample indexes\n",
        "  random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
        "\n",
        "  # 5. Setup plot\n",
        "  plt.figure(figsize=(16, 8))\n",
        "\n",
        "  # 6. Loop through random indexes and plot them with matplotlib\n",
        "  for i, targ_sample in enumerate(random_samples_idx):\n",
        "    targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
        "\n",
        "    # 7. Adjust tensor dimensions for plotting\n",
        "    targ_image_adjust = targ_image.permute(1, 2, 0) # [color_channels, height, width] -> [height, width, color_channels]\n",
        "\n",
        "    # Plot adjusted samples\n",
        "    plt.subplot(1, n, i+1)\n",
        "    plt.imshow(targ_image_adjust)\n",
        "    plt.axis(\"off\")\n",
        "    if classes:\n",
        "      title = f\"Class: {classes[targ_label]}\"\n",
        "      if display_shape:\n",
        "        title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
        "    plt.title(title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "o5PXoypVzUKD",
        "outputId": "fc888bee-aa54-4a17-ccc7-1f64aaeb5446"
      },
      "outputs": [],
      "source": [
        "# Display random images from the ImageFolder created Dataset\n",
        "display_random_images(train_data,\n",
        "                      n=5, \n",
        "                      classes=class_names,\n",
        "                      seed=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "3g5Lzh50zYvz",
        "outputId": "16458656-dfa0-41bb-bfeb-7475fb8ebc54"
      },
      "outputs": [],
      "source": [
        "# Display random images from the ImageFolderCustom Dataset\n",
        "display_random_images(train_data_custom,\n",
        "                      n=5,\n",
        "                      classes=class_names,\n",
        "                      seed=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0qALAmq08Wb"
      },
      "source": [
        "### 5.4 Turn custom loaded images into `DataLoader`'s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzpVhY2d1y-C",
        "outputId": "6c6eaf43-1c18-457f-8565-483fe52c2662"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "train_dataloader_custom = DataLoader(dataset=train_data_custom,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     num_workers=NUM_WORKERS,\n",
        "                                     shuffle=True)\n",
        "\n",
        "test_dataloader_custom = DataLoader(dataset=test_data_custom,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    num_workers=NUM_WORKERS,\n",
        "                                    shuffle=False)\n",
        "\n",
        "train_dataloader_custom, test_dataloader_custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader_custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bDcpqQ92NLA",
        "outputId": "8f351037-3a13-489b-9e14-f494bba10ec1"
      },
      "outputs": [],
      "source": [
        "# Get image and label from custom datloader\n",
        "img_custom, label_custom = next(iter(train_dataloader_custom))\n",
        "\n",
        "# Print out the shapes\n",
        "img_custom.shape, label_custom.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aml-Oedz2NF9"
      },
      "source": [
        "## 6. Other forms of transforms (data augmentation)\n",
        "\n",
        "Data augmentation is the process of artificially adding diversity to your training data.\n",
        "\n",
        "In the case of image data, this may mean applying various image transformations to the training images.\n",
        "\n",
        "This practice hopefully results in a model that's more generalizable to unseen data.\n",
        "\n",
        "Let's take a look at one particular type of data augmentation used to train PyTorch vision models to state of the art levels...\n",
        "\n",
        "Blog post: https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break-down-of-key-accuracy-improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNjK7kTmb3yc"
      },
      "outputs": [],
      "source": [
        "# Let's look at trivailaugment - https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#trivialaugmentwide \n",
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "                                      transforms.Resize(size=(224, 224)),\n",
        "                                      transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
        "                                      transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                      transforms.Resize(size=(224, 224)),\n",
        "                                      transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaVgnFTdd4gZ",
        "outputId": "9b102bf8-9270-41f4-c1a6-1248ba05cbcf"
      },
      "outputs": [],
      "source": [
        "image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kg4urOgdxgd",
        "outputId": "f909fa30-16f2-4b66-ccfa-574551c17aed"
      },
      "outputs": [],
      "source": [
        "# Get all image paths\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "image_path_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "gnLJC5nGd-fD",
        "outputId": "9eb33ea5-5c5b-4edc-f486-15919200bc04"
      },
      "outputs": [],
      "source": [
        "# Plot random transformed images\n",
        "plot_transformed_images(\n",
        "    image_paths=image_path_list,\n",
        "    transform=train_transform,\n",
        "    n=3,\n",
        "    seed=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-twGKuoeIiY"
      },
      "source": [
        "## 7. Model 0: TinyVGG without data augmentation \n",
        "\n",
        "Let's replicate TinyVGG architecture from the CNN Explainer website: https://poloclub.github.io/cnn-explainer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM15BWGsfj2n"
      },
      "source": [
        "### 7.1 Creating transforms and loading data for Model 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYph4hsJf2Fl"
      },
      "outputs": [],
      "source": [
        "# Create simple transform\n",
        "simple_transform = transforms.Compose([\n",
        "                                       transforms.Resize(size=(64, 64)),\n",
        "                                       transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1SRKg2ngIH-"
      },
      "outputs": [],
      "source": [
        "# 1. Load and transform data\n",
        "from torchvision import datasets\n",
        "train_data_simple = datasets.ImageFolder(root=train_dir,\n",
        "                                         transform=simple_transform)\n",
        "test_data_simple = datasets.ImageFolder(root=test_dir,\n",
        "                                        transform=simple_transform)\n",
        "\n",
        "# 2. Turn the datasets into DataLoaders\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup batch size and number of works\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Create DataLoader's\n",
        "train_dataloader_simple = DataLoader(dataset=train_data_simple,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     shuffle=True, \n",
        "                                     num_workers=NUM_WORKERS)\n",
        "test_dataloader_simple = DataLoader(dataset=test_data_simple,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    shuffle=False,\n",
        "                                    num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yyqrd8g1Ov"
      },
      "source": [
        "### 7.2 Create TinyVGG model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4FwoED8hxPK"
      },
      "outputs": [],
      "source": [
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"\n",
        "  Model architecture copying TinyVGG from CNN Explainer: https://poloclub.github.io/cnn-explainer/\n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int) -> None:\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2) # default stride value is same as kernel_size\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2) # default stride value is same as kernel_size\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(), \n",
        "        nn.Linear(in_features=hidden_units*13*13,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    # print(x.shape)\n",
        "    x = self.conv_block_2(x)\n",
        "    # print(x.shape)\n",
        "    x = self.classifier(x)\n",
        "    # print(x.shape)\n",
        "    return x\n",
        "    # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # benefits from operator fusion: https://horace.io/brrr_intro.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBGw6X08jwbj",
        "outputId": "6d3f79bc-bc22-48bc-a975-b6c3f9fa6601"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape=3, # number of color channels in our image data\n",
        "                  hidden_units=10, \n",
        "                  output_shape=len(class_names)).to(device)\n",
        "\n",
        "model_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe8Ktz1wkFrt"
      },
      "source": [
        "### 7.3 Try a forward pass on a single image (to test the model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hYLOx3ClBqv",
        "outputId": "a6cb53e0-d37f-4394-b377-b3049006802c"
      },
      "outputs": [],
      "source": [
        "# Get a single image batch\n",
        "image_batch, label_batch = next(iter(train_dataloader_simple))\n",
        "image_batch.shape, label_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svpu2ZAelLHc",
        "outputId": "0b9809be-86b8-4482-9739-60864d37e08a"
      },
      "outputs": [],
      "source": [
        "# Try a forward pass\n",
        "model_0(image_batch.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDd5YLvslu-M"
      },
      "source": [
        "### 7.4 Use `torchinfo` to get an idea of the shapes going through our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edJ__PYGjZRB",
        "outputId": "a31b7180-8bdd-48f8-cb2e-2c3022ffff44"
      },
      "outputs": [],
      "source": [
        "# Install torchinfo, import if it's available\n",
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "summary(model_0, input_size=[1, 3, 64, 64])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpRg90E1jj5V"
      },
      "source": [
        "### 7.5 Create train and test loops functions\n",
        "\n",
        "* `train_step()` - takes in a model and dataloader and trains the model on the dataloader.\n",
        "* `test_step()` - takes in a model and dataloader and evaluates the model on the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgMSBTqblVcC"
      },
      "outputs": [],
      "source": [
        "# Create train_step()\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer:torch.optim.Optimizer,\n",
        "               device=device):\n",
        "  # Put the model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Send data to the target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X) # output model logits\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate accuracy metric\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class==y).sum().item()/len(y_pred)\n",
        "  \n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader) \n",
        "  return train_loss, train_acc "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8AezGV-mwFd"
      },
      "outputs": [],
      "source": [
        "# Create a test step\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device=device):\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0,  0\n",
        "\n",
        "  # Turn on inference mode\n",
        "  with torch.inference_mode():\n",
        "    # Loop through DataLoader batches\n",
        "    for batch, (X, y) in enumerate(dataloader): \n",
        "      # Send data to the target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X)\n",
        "\n",
        "      # 2. Calculate the loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate the accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wijbnuKCn384"
      },
      "source": [
        "### 7.6 Creating a `train()` function to combine `train_step()` and `test_step()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWStjh81ouUv"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Create a train function that takes in various model parameters + optimizer + dataloaders + loss function\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader,\n",
        "          test_dataloader,\n",
        "          optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5, \n",
        "          device=device):\n",
        "  \n",
        "  # 2. Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": []}\n",
        "  \n",
        "  # 3. Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                       dataloader=train_dataloader,\n",
        "                                       loss_fn=loss_fn,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device)\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "                                    dataloader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    device=device)\n",
        "    \n",
        "    # 4. Print out what's happening\n",
        "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
        "\n",
        "    # 5. Update results dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "  \n",
        "  # 6. Return the filled results at the end of the epochs\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlf8xNdeqjFB"
      },
      "source": [
        "### 7.7 Train and evaluate model 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "a570402de0864c60a54fa7e32105fef2",
            "d1f0525bac574b9891cb7c60dda58c91",
            "8ccb83d85e0c4ef295e9a324d4e018d2",
            "9802b65eed134ca3b8d70cde34c9c192",
            "f80f3eaa1ece4044844cd046076f75fe",
            "8328eb6b15eb4744adf65124e91a021d",
            "996eb0d465094542a232c75fa4d87d9d",
            "8134e67b271b4fd8be6bd8ded1fcf673",
            "2179e9861cb94ae9aa49480dca7937f6",
            "3e34a89c9bef43b4a03d9f1d915d4076",
            "c4b92476155444f19cfc2b55374ef545"
          ]
        },
        "id": "PU_f4DZVgjOP",
        "outputId": "d7a33b12-9988-4759-e5b0-9f507c15955b"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42) \n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Recreate an instance of TinyVGG\n",
        "model_0 = TinyVGG(input_shape=3, # number of color channels of our target images\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "# Setup loss function and optimizer \n",
        "loss_fn = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
        "                             lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer() \n",
        "\n",
        "# Train model_0\n",
        "model_0_results = train(model=model_0,\n",
        "                        train_dataloader=train_dataloader_simple,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JObuydR9iC_x",
        "outputId": "b68d65dd-718c-45c4-9219-ee1a27d13885"
      },
      "outputs": [],
      "source": [
        "model_0_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx6sQq1pix9a"
      },
      "source": [
        "### 7.8 Plot the loss curves of Model 0\n",
        "\n",
        "A **loss curve** is a way of tracking your model's progress over time.\n",
        "\n",
        "A good guide for different loss curves can be seen here: https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLYF_aGKjRUZ",
        "outputId": "ecef2369-16c5-458a-e680-7696c9a83158"
      },
      "outputs": [],
      "source": [
        "# Get the model_0_results keys\n",
        "model_0_results.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KseocaD1jidP"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "  \"\"\"Plots training curves of a results dictionary.\"\"\"\n",
        "  # Get the loss values of the results dictionary(training and test)\n",
        "  loss = results[\"train_loss\"]\n",
        "  test_loss = results[\"test_loss\"]\n",
        "\n",
        "  # Get the accuracy values of the results dictionary (training and test)\n",
        "  accuracy = results[\"train_acc\"]\n",
        "  test_accuracy = results[\"test_acc\"]\n",
        "\n",
        "  # Figure out how mnay epochs there were\n",
        "  epochs = range(len(results[\"train_loss\"]))\n",
        "\n",
        "  # Setup a plot\n",
        "  plt.figure(figsize=(15, 7))\n",
        "\n",
        "  # Plot the loss\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs, loss, label=\"train_loss\")\n",
        "  plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend() \n",
        "\n",
        "  # Plot the accuracy\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "  plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Qn2sy4p9ktXU",
        "outputId": "301355ec-5993-489c-c3ec-33ba2b4bdc50"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(model_0_results) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNoK5meokvta"
      },
      "source": [
        "## 8. What should an ideal loss curve look like?\n",
        "\n",
        "https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic\n",
        "\n",
        "A loss curve is one of the most helpful ways to troubleshoot a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eji5pwgDLFZq"
      },
      "source": [
        "## 9. Model 1: TinyVGG with Data Augmentation\n",
        "\n",
        "Now let's try another modelling experiment this time using the same model as before with some data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofvPBsl2QE1z"
      },
      "source": [
        "### 9.1 Create transform with data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOkZsHJvQcgh"
      },
      "outputs": [],
      "source": [
        "# Create training transform with TriviailAugment\n",
        "from torchvision import transforms\n",
        "train_transform_trivial = transforms.Compose([\n",
        "                                               transforms.Resize(size=(64, 64)),\n",
        "                                               transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
        "                                               transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform_simple = transforms.Compose([\n",
        "                                            transforms.Resize(size=(64, 64)),\n",
        "                                            transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSvUJ92nQ_VF"
      },
      "source": [
        "### 9.2 Create train and test `Dataset`'s and `DataLoader`'s with data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v03wShiKRE6J"
      },
      "outputs": [],
      "source": [
        "# Turn image folders into Datasets\n",
        "from torchvision import datasets\n",
        "train_data_augmented = datasets.ImageFolder(root=train_dir,\n",
        "                                            transform=train_transform_trivial)\n",
        "test_data_simple = datasets.ImageFolder(root=test_dir,\n",
        "                                        transform=test_transform_simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-gQIKDyRXD3"
      },
      "outputs": [],
      "source": [
        "# Turn our Datasets into DataLoaders\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_dataloader_augmented = DataLoader(dataset=train_data_augmented,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                        num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader_simple = DataLoader(dataset=test_data_simple,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    shuffle=False,\n",
        "                                    num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tWM5in0WFDh"
      },
      "source": [
        "### 9.3 Construct and train model 1\n",
        "\n",
        "This time we'll be using the same model architecture except this time we've augmented the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIlVs8MzXYfL",
        "outputId": "e718ba4b-28ec-49f7-adda-699ac304876d"
      },
      "outputs": [],
      "source": [
        "# Create model_1 and send it to the target device\n",
        "torch.manual_seed(42)\n",
        "model_1 = TinyVGG(input_shape=3,\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data_augmented.classes)).to(device)\n",
        "\n",
        "model_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WutIydquXsDA"
      },
      "source": [
        "Wonderful! Now we've a model and dataloaders, let's create a loss function and an optimizer and call upon our `train()` function to train and evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "7c84df101a1f4c8e82c6b2cc97c6aefd",
            "740b7497399f42708aa95fbee4c3be31",
            "bbb7e0ea4348404f944f9deeb01367e7",
            "8796046aa0e54b4a9e2dc10b550c101e",
            "93abd8502bb0416e918d2fc845faa7be",
            "b5884bf8caf24179b28fedc3c949ee9a",
            "a609483f31fb4fe899c78583f5af5d80",
            "3e28f4de246f4b96b3c1ecc206dfca10",
            "f0b07fa6dda8401699c768a0ce71cf98",
            "19ac6240f3bf47918c5646dea7a33dd8",
            "25c662bad81f404f8f24de2af292d5e5"
          ]
        },
        "id": "OPxK_jUaXowB",
        "outputId": "8e7f2465-dff8-4c0e-b75e-56fcf578f89e"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Setup loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_1.parameters(),\n",
        "                             lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Train model 1\n",
        "model_1_results = train(model=model_1,\n",
        "                        train_dataloader=train_dataloader_augmented,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"Total training time for model_1: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ7xxhRzYmXz"
      },
      "source": [
        "### 9.4 Plot the loss curves of model 1\n",
        "\n",
        "A loss curve helps you evaluate your models performance overtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "k1U01UtbY4Tp",
        "outputId": "f7323ec0-b989-4dc0-99e3-091ab7aecf40"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(model_1_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGEchcnT-Ox4"
      },
      "source": [
        "## 10. Compare model results\n",
        "\n",
        "After evaluating our modelling experiments on their own, it's important to compare them to each other.\n",
        "\n",
        "There's a few different ways to do this:\n",
        "1. Hard coding (what we're doing)\n",
        "2. PyTorch + Tensorboard - https://pytorch.org/docs/stable/tensorboard.html\n",
        "3. Weights & Biases - https://wandb.ai/site/experiment-tracking \n",
        "4. MLFlow - https://mlflow.org/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "smCbJIz4-qck",
        "outputId": "954f69b3-3980-4f90-c266-34f9355ceb64"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "model_0_df = pd.DataFrame(model_0_results)\n",
        "model_1_df = pd.DataFrame(model_1_results)\n",
        "model_0_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "mKZWJ4Q7-qKq",
        "outputId": "17a5e1f0-158e-4e2c-ec55-4f78cc0e05b0"
      },
      "outputs": [],
      "source": [
        "# Setup a plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(model_0_df))\n",
        "\n",
        "# Plot train loss\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot test loss\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Test Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot train accuracy\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Train Acc\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot test accuracy\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Test Acc\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LusBmaC_p2A"
      },
      "source": [
        "## 11. Making a prediction on a custom image\n",
        "\n",
        "Although we've trained a model on custom data... how do you make a prediction on a sample/image that's not in either training or testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQpUbQ_0BpRl",
        "outputId": "9301e9bd-6ee0-40f8-da3c-2d96bd9b5247"
      },
      "outputs": [],
      "source": [
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "  with open(custom_image_path, \"wb\") as f:\n",
        "    # When downloading from GitHub, need to use the \"raw\" file link\n",
        "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "    print(f\"Downloading {custom_image_path}...\")\n",
        "    f.write(request.content)\n",
        "else:\n",
        "  print(f\"{custom_image_path} already exists, skipping download...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpiyZTf5CmDQ"
      },
      "source": [
        "### 11.1 Loading in a custom image with PyTorch\n",
        "\n",
        "We have to make sure our custom image is in the same format as the data our model was trained on.\n",
        "\n",
        "* In tensor form with datatype (torch.float32)\n",
        "* Of shape 64x64x3\n",
        "* On the right device\n",
        "\n",
        "We can read an image into PyTorch using - https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwzv_xLPD_82",
        "outputId": "c9a48209-5735-4cf5-b0a4-ad695779a594"
      },
      "outputs": [],
      "source": [
        "custom_image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwBdGkp5DUs1",
        "outputId": "f1034f9a-96f6-413d-e279-f5f907ca8422"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "print(f\"Custom image tensor:\\n {custom_image_uint8}\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\")\n",
        "print(f\"Custom image datatype: {custom_image_uint8.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "hAFjtLW7D7tq",
        "outputId": "7bb0f7fe-3872-4dc2-e45e-82b554a2bd41"
      },
      "outputs": [],
      "source": [
        "plt.imshow(custom_image_uint8.permute(1, 2, 0));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u4v_Px8EF7y"
      },
      "source": [
        "### 11.2 Making a prediction on a custom image with a trained PyTorch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "CyQgWaDzFH75",
        "outputId": "72d18e9d-8bc8-4cba-f42f-cc16770af2cf"
      },
      "outputs": [],
      "source": [
        "# Try to make a prediction on an image in uint8 format\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    #transforms.ToTensor()\n",
        "])\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  model_1(transform(custom_image_uint8).float().unsqueeze(0).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4EWUc2_FTtT",
        "outputId": "84155b80-08a9-4006-d64a-ac0a3281a7bf"
      },
      "outputs": [],
      "source": [
        "# Load in the custom image and convert to torch.float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32) / 255.\n",
        "custom_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "bSWuF_INFmSQ",
        "outputId": "b39f5056-9d19-4e13-8f97-e3d6a6cec990"
      },
      "outputs": [],
      "source": [
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  model_1(custom_image.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_vB7VnHFqis",
        "outputId": "2a3751ca-e817-4d32-87e8-4adcb1450100"
      },
      "outputs": [],
      "source": [
        "# Create transform pipeline to resize image\n",
        "from torchvision import transforms\n",
        "custom_image_transform = transforms.Compose([\n",
        "                                             transforms.Resize(size=(64, 64))\n",
        "])\n",
        "\n",
        "# Transfrom target image\n",
        "custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out the shapes\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"Transformed shape: {custom_image_transformed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "ELQ_SMXvGTwx",
        "outputId": "3c65b4d0-7394-45cc-fabe-61ac98cb0666"
      },
      "outputs": [],
      "source": [
        "plt.imshow(custom_image_transformed.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "HuhrmhNEGbdp",
        "outputId": "31c99434-02d7-45e3-81b4-0a2e4f9bd108"
      },
      "outputs": [],
      "source": [
        "# This will error: image not on right device\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  custom_image_pred = model_1(custom_image_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "WqXcUA4KGwzc",
        "outputId": "e178d64f-73ac-4249-9c9e-cdde602d2ff1"
      },
      "outputs": [],
      "source": [
        "# This will error: no batch size\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  custom_image_pred = model_1(custom_image_transformed.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm9Ih5irHGvi",
        "outputId": "2f9cdd4d-7722-4c29-c8eb-3cd7c8c0e45c"
      },
      "outputs": [],
      "source": [
        "custom_image_transformed.shape, custom_image_transformed.unsqueeze(0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBGeJAzTHPxJ",
        "outputId": "a44c5c6c-d0c4-4541-ed44-386b123f8813"
      },
      "outputs": [],
      "source": [
        "# This should this work? (added a batch size...)\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  custom_image_pred = model_1(custom_image_transformed.unsqueeze(0).to(device))\n",
        "custom_image_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HcIEn-HsWL"
      },
      "source": [
        "Note, to make a prediction on a custom image we had to:\n",
        "* Load the image and turn it into a tensor\n",
        "* Make sure the image was the same datatype as the model (torch.float32)\n",
        "* Make sure the image was the same shape as the data the model was trained on (3, 64, 64) with a batch size... (1, 3, 64, 64)\n",
        "* Make sure the image was on the same device as our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmvLvyfGHoSP",
        "outputId": "2311de74-3715-43be-aecc-21c8c5e886e5"
      },
      "outputs": [],
      "source": [
        "# Convert logits -> prediction probabilities\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "custom_image_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufjnNcmdJCP7",
        "outputId": "a71d307a-c535-40e4-f2f9-dc9a103d9d44"
      },
      "outputs": [],
      "source": [
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1).cpu()\n",
        "custom_image_pred_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8Lxskhu0JTBM",
        "outputId": "c5d8366e-59f6-409f-c99a-a37596b2642e"
      },
      "outputs": [],
      "source": [
        "class_names[custom_image_pred_label]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWmhUjpJWUf"
      },
      "source": [
        "### 11.3 Putting custom image prediction together: building a function\n",
        "\n",
        "Ideal outcome:\n",
        "\n",
        "A function where we pass an image path to and have our model predict on that image and plot the image + prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXkiI6L5JlI9"
      },
      "outputs": [],
      "source": [
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str] = None,\n",
        "                        transform=None,\n",
        "                        device=device):\n",
        "  \"\"\"Makes a prediction on a target image with a trained model and plots the image and prediction.\"\"\"\n",
        "  # Load in the image\n",
        "  target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
        "\n",
        "  # Divide the image pixel values by 255 to get them between [0, 1]\n",
        "  target_image = target_image / 255.\n",
        "\n",
        "  # Transform if necessary\n",
        "  if transform:\n",
        "    target_image = transform(target_image)\n",
        "\n",
        "  # Make sure the model is on the target device\n",
        "  model.to(device)\n",
        "\n",
        "  # Turn on eval/inference mode and make a prediction\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Add an extra dimension to the image (this is the batch dimension, e.g. our model will predict on batches of 1x image)\n",
        "    target_image = target_image.unsqueeze(0)\n",
        "\n",
        "    # Make a prediction on the image with an extra dimension\n",
        "    target_image_pred = model(target_image.to(device)) # make sure the target image is on the right device\n",
        "\n",
        "  # Convert logits -> prediction probabilities\n",
        "  target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "  # Convert predction probabilities -> prediction labels\n",
        "  target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "  # Plot the image alongside the prediction and prediction probability\n",
        "  plt.imshow(target_image.squeeze().permute(1, 2, 0)) # remove batch dimension and rearrange shape to be HWC\n",
        "  if class_names:\n",
        "    title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "  else:\n",
        "    title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "  plt.title(title)\n",
        "  plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "3dCZntehMJlr",
        "outputId": "21c026d9-f186-45bc-b23f-3d155fc6b6d5"
      },
      "outputs": [],
      "source": [
        "# Pred on our custom image\n",
        "pred_and_plot_image(model=model_1,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names,\n",
        "                    transform=custom_image_transform,\n",
        "                    device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKw2UZ0MfMc"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "For all exercises and extra-curriculum, see here: https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNf1RdC5gDzDQoO0gcDUDlF",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "04_pytorch_custom_datasets_video.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19ac6240f3bf47918c5646dea7a33dd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2179e9861cb94ae9aa49480dca7937f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25c662bad81f404f8f24de2af292d5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e28f4de246f4b96b3c1ecc206dfca10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e34a89c9bef43b4a03d9f1d915d4076": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "740b7497399f42708aa95fbee4c3be31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5884bf8caf24179b28fedc3c949ee9a",
            "placeholder": "",
            "style": "IPY_MODEL_a609483f31fb4fe899c78583f5af5d80",
            "value": "100%"
          }
        },
        "7c84df101a1f4c8e82c6b2cc97c6aefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_740b7497399f42708aa95fbee4c3be31",
              "IPY_MODEL_bbb7e0ea4348404f944f9deeb01367e7",
              "IPY_MODEL_8796046aa0e54b4a9e2dc10b550c101e"
            ],
            "layout": "IPY_MODEL_93abd8502bb0416e918d2fc845faa7be"
          }
        },
        "8134e67b271b4fd8be6bd8ded1fcf673": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8328eb6b15eb4744adf65124e91a021d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8796046aa0e54b4a9e2dc10b550c101e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19ac6240f3bf47918c5646dea7a33dd8",
            "placeholder": "",
            "style": "IPY_MODEL_25c662bad81f404f8f24de2af292d5e5",
            "value": " 5/5 [00:10&lt;00:00,  2.36s/it]"
          }
        },
        "8ccb83d85e0c4ef295e9a324d4e018d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8134e67b271b4fd8be6bd8ded1fcf673",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2179e9861cb94ae9aa49480dca7937f6",
            "value": 5
          }
        },
        "93abd8502bb0416e918d2fc845faa7be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9802b65eed134ca3b8d70cde34c9c192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e34a89c9bef43b4a03d9f1d915d4076",
            "placeholder": "",
            "style": "IPY_MODEL_c4b92476155444f19cfc2b55374ef545",
            "value": " 5/5 [00:10&lt;00:00,  2.13s/it]"
          }
        },
        "996eb0d465094542a232c75fa4d87d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a570402de0864c60a54fa7e32105fef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1f0525bac574b9891cb7c60dda58c91",
              "IPY_MODEL_8ccb83d85e0c4ef295e9a324d4e018d2",
              "IPY_MODEL_9802b65eed134ca3b8d70cde34c9c192"
            ],
            "layout": "IPY_MODEL_f80f3eaa1ece4044844cd046076f75fe"
          }
        },
        "a609483f31fb4fe899c78583f5af5d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5884bf8caf24179b28fedc3c949ee9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb7e0ea4348404f944f9deeb01367e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e28f4de246f4b96b3c1ecc206dfca10",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0b07fa6dda8401699c768a0ce71cf98",
            "value": 5
          }
        },
        "c4b92476155444f19cfc2b55374ef545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1f0525bac574b9891cb7c60dda58c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8328eb6b15eb4744adf65124e91a021d",
            "placeholder": "",
            "style": "IPY_MODEL_996eb0d465094542a232c75fa4d87d9d",
            "value": "100%"
          }
        },
        "f0b07fa6dda8401699c768a0ce71cf98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f80f3eaa1ece4044844cd046076f75fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
