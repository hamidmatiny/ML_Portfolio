{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: X=torch.Size([10000, 7]), y=torch.Size([10000, 4])\n"
     ]
    }
   ],
   "source": [
    "# Data generation\n",
    "def generate_data(num_samples=10000):\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        a = random.randint(1, 999)\n",
    "        b = random.randint(1, 999)\n",
    "        question = f\"{a}+{b}\"\n",
    "        answer = str(a + b)\n",
    "        data.append((question, answer))\n",
    "    return data\n",
    "\n",
    "# Vocabulary\n",
    "chars = '0123456789+'\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(text, max_len):\n",
    "    encoded = [char_to_idx[c] for c in text]\n",
    "    return encoded + [0] * (max_len - len(encoded))\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([idx_to_char[i] for i in indices if i != 0])\n",
    "\n",
    "# Prepare data\n",
    "data = generate_data()\n",
    "max_input_len = 7  # \"999+999\"\n",
    "max_output_len = 4  # \"1998\"\n",
    "\n",
    "X = torch.tensor([encode(q, max_input_len) for q, a in data])\n",
    "y = torch.tensor([encode(a, max_output_len) for q, a in data])\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], dim=2)))\n",
    "        attention = torch.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        context = torch.bmm(attention.unsqueeze(1), encoder_outputs)\n",
    "        return context.squeeze(1), attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(input.unsqueeze(1))\n",
    "        context, _ = self.attention(hidden[-1], encoder_outputs)\n",
    "        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        prediction = self.out(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, hidden_size)\n",
    "        self.decoder = Decoder(vocab_size, hidden_size)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        outputs = []\n",
    "        input = torch.zeros(src.size(0), dtype=torch.long, device=src.device)\n",
    "        \n",
    "        for t in range(trg.size(1)):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs.append(output)\n",
    "            input = trg[:, t]\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 1.4373\n",
      "Epoch 10/20, Loss: 1.2674\n",
      "Epoch 15/20, Loss: 1.1438\n",
      "Epoch 20/20, Loss: 1.0056\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = Seq2Seq(vocab_size, 128)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X, batch_y)\n",
    "        loss = criterion(output.reshape(-1, vocab_size), batch_y.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(X_train)*batch_size:.4f}')\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123+456 = 6181 (actual: 579)\n",
      "789+111 = 9121 (actual: 900)\n",
      "50+25 = 7132 (actual: 75)\n",
      "999+1 = 1134 (actual: 1000)\n"
     ]
    }
   ],
   "source": [
    "def predict(model, question):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode input\n",
    "        input_seq = torch.tensor([encode(question, max_input_len)])\n",
    "        \n",
    "        # Get encoder outputs\n",
    "        encoder_outputs, hidden, cell = model.encoder(input_seq)\n",
    "        \n",
    "        # Decode step by step\n",
    "        outputs = []\n",
    "        input_token = torch.zeros(1, dtype=torch.long)\n",
    "        \n",
    "        for _ in range(max_output_len):\n",
    "            output, hidden, cell = model.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "            predicted = output.argmax(dim=1)\n",
    "            outputs.append(predicted.item())\n",
    "            input_token = predicted\n",
    "            \n",
    "            if predicted.item() == 0:  # Stop at padding\n",
    "                break\n",
    "        \n",
    "        return decode(outputs)\n",
    "\n",
    "# Test on some examples\n",
    "test_examples = [\"123+456\", \"789+111\", \"50+25\", \"999+1\"]\n",
    "for example in test_examples:\n",
    "    prediction = predict(model, example)\n",
    "    actual = str(eval(example))\n",
    "    print(f\"{example} = {prediction} (actual: {actual})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction cell\n",
    "user_input = input(\"Enter an addition problem (e.g., '123+456'): \")\n",
    "try:\n",
    "    prediction = predict(model, user_input)\n",
    "    actual = str(eval(user_input))\n",
    "    print(f\"\\nModel prediction: {user_input} = {prediction}\")\n",
    "    print(f\"Actual answer: {actual}\")\n",
    "    print(f\"Correct: {'✓' if prediction == actual else '✗'}\")\n",
    "except:\n",
    "    print(\"Invalid input. Please use format like '123+456'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
