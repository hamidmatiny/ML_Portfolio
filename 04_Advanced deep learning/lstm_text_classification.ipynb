{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Text Classification - IMDB Sentiment Analysis\n",
        "\n",
        "This notebook demonstrates LSTM (Long Short-Term Memory) networks for text classification using PyTorch.\n",
        "\n",
        "## Topics Covered:\n",
        "- Text preprocessing and tokenization\n",
        "- Building vocabulary from text data\n",
        "- LSTM architecture for sequence modeling\n",
        "- Bidirectional LSTM for better context understanding\n",
        "- Training and evaluation of text classification models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Install datasets library if not available (uncomment if needed)\n",
        "# !pip install datasets\n",
        "\n",
        "# Import helper functions if available\n",
        "import sys\n",
        "sys.path.append('../03_Deep Learning Fundamentals')\n",
        "try:\n",
        "    from helper_functions import accuracy_fn, plot_loss_curves, set_seeds\n",
        "except ImportError:\n",
        "    # Define helper functions if not available\n",
        "    def accuracy_fn(y_true, y_pred):\n",
        "        correct = torch.eq(y_true, y_pred).sum().item()\n",
        "        acc = (correct / len(y_pred)) * 100\n",
        "        return acc\n",
        "    \n",
        "    def set_seeds(seed=42):\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    def plot_loss_curves(results):\n",
        "        loss = results[\"train_loss\"]\n",
        "        test_loss = results[\"test_loss\"]\n",
        "        accuracy = results[\"train_acc\"]\n",
        "        test_accuracy = results[\"test_acc\"]\n",
        "        epochs = range(len(results[\"train_loss\"]))\n",
        "        \n",
        "        plt.figure(figsize=(15, 7))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, loss, label=\"train_loss\")\n",
        "        plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "        plt.title(\"Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "        plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.legend()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "set_seeds(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and Load IMDB Dataset\n",
        "# Using datasets library for easier IMDB access\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    print(\"Using HuggingFace datasets library...\")\n",
        "    # Load IMDB dataset\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "    train_data = [(item['label'], item['text']) for item in dataset['train']]\n",
        "    test_data = [(item['label'], item['text']) for item in dataset['test']]\n",
        "    # Convert labels: 1 -> 'pos', 0 -> 'neg' to match expected format\n",
        "    train_data = [('pos' if label == 1 else 'neg', text) for label, text in train_data]\n",
        "    test_data = [('pos' if label == 1 else 'neg', text) for label, text in test_data]\n",
        "except ImportError:\n",
        "    print(\"HuggingFace datasets not available, trying torchtext...\")\n",
        "    try:\n",
        "        # Try newer torchtext API\n",
        "        from torchtext.datasets import IMDB\n",
        "        from torchtext.data.utils import get_tokenizer\n",
        "        from torchtext.vocab import build_vocab_from_iterator\n",
        "        \n",
        "        # For newer torchtext, we need to use it differently\n",
        "        # This is a workaround - downloading manually or using alternative\n",
        "        print(\"Please install datasets: pip install datasets\")\n",
        "        raise ImportError(\"Please install datasets library: pip install datasets\")\n",
        "    except:\n",
        "        # Fallback: Use a simple synthetic dataset for demonstration\n",
        "        print(\"Creating synthetic dataset for demonstration...\")\n",
        "        train_data = [\n",
        "            ('pos', 'This movie was absolutely fantastic! I loved every minute of it. Great acting and storyline.'),\n",
        "            ('neg', 'Terrible movie, waste of time. The acting was poor and the plot made no sense.'),\n",
        "            ('pos', 'Amazing cinematography and great performances from all actors. Highly recommend!'),\n",
        "            ('neg', 'Boring and predictable. I fell asleep halfway through.'),\n",
        "        ] * 6250  # Scale to ~25k samples\n",
        "        test_data = [\n",
        "            ('pos', 'Wonderful film with excellent direction and compelling narrative.'),\n",
        "            ('neg', 'Disappointing and poorly executed. Not worth watching.'),\n",
        "        ] * 1250  # Scale to ~2.5k samples\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "print(f\"\\nSample review (first 200 chars):\")\n",
        "print(train_data[0][1][:200])\n",
        "print(f\"\\nLabel: {train_data[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Vocabulary\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def build_vocab(data, min_freq=2):\n",
        "    \"\"\"Build vocabulary from dataset\"\"\"\n",
        "    counter = Counter()\n",
        "    for label, text in data:\n",
        "        counter.update(tokenizer(text.lower()))\n",
        "    \n",
        "    # Filter by minimum frequency\n",
        "    filtered_counter = {word: count for word, count in counter.items() if count >= min_freq}\n",
        "    \n",
        "    # Create vocabulary dictionary with special tokens\n",
        "    # Special tokens: <pad>=0, <unk>=1\n",
        "    vocab_dict = {'<pad>': 0, '<unk>': 1}\n",
        "    \n",
        "    # Add words sorted by frequency (most common first)\n",
        "    sorted_words = sorted(filtered_counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    for idx, (word, count) in enumerate(sorted_words, start=2):\n",
        "        vocab_dict[word] = idx\n",
        "    \n",
        "    # Create reverse mapping (index to word)\n",
        "    idx_to_word = {idx: word for word, idx in vocab_dict.items()}\n",
        "    \n",
        "    # Create a simple vocabulary class\n",
        "    class Vocab:\n",
        "        def __init__(self, word_to_idx, idx_to_word):\n",
        "            self.word_to_idx = word_to_idx\n",
        "            self.idx_to_word = idx_to_word\n",
        "        \n",
        "        def __getitem__(self, word):\n",
        "            return self.word_to_idx.get(word, self.word_to_idx['<unk>'])\n",
        "        \n",
        "        def __len__(self):\n",
        "            return len(self.word_to_idx)\n",
        "        \n",
        "        def get_itos(self):\n",
        "            \"\"\"Get index to string mapping\"\"\"\n",
        "            return [self.idx_to_word[i] for i in range(len(self.idx_to_word))]\n",
        "    \n",
        "    vocab = Vocab(vocab_dict, idx_to_word)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_data, min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Sample tokens: {vocab.get_itos()[:20]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset Class\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, vocab, tokenizer, max_length=500):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.data[idx]\n",
        "        # Convert label: 'pos' -> 1, 'neg' -> 0\n",
        "        label = 1 if label == 'pos' else 0\n",
        "        \n",
        "        # Tokenize and convert to indices\n",
        "        tokens = self.tokenizer(text.lower())\n",
        "        indices = [self.vocab[token] for token in tokens]\n",
        "        \n",
        "        # Truncate if too long\n",
        "        if len(indices) > self.max_length:\n",
        "            indices = indices[:self.max_length]\n",
        "        \n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IMDBDataset(train_data, vocab, tokenizer, max_length=500)\n",
        "test_dataset = IMDBDataset(test_data, vocab, tokenizer, max_length=500)\n",
        "\n",
        "# Custom collate function to handle variable-length sequences\n",
        "def collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    # Pad sequences to same length\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=vocab['<pad>'])\n",
        "    labels = torch.stack(labels)\n",
        "    return sequences_padded, labels\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define LSTM Model\n",
        "class LSTMSentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, \n",
        "                 dropout=0.3, num_classes=2):\n",
        "        super(LSTMSentimentClassifier, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<pad>'])\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                           bidirectional=True)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        # *2 because bidirectional LSTM outputs 2*hidden_dim\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        # Use the last hidden state from both directions\n",
        "        # hidden shape: (num_layers * 2, batch_size, hidden_dim)\n",
        "        # Take the last layer's hidden states from both directions\n",
        "        forward_hidden = hidden[-2, :, :]  # Last forward direction\n",
        "        backward_hidden = hidden[-1, :, :]  # Last backward direction\n",
        "        combined_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
        "        \n",
        "        # Alternative: Use the last output from LSTM\n",
        "        # combined_hidden = lstm_out[:, -1, :]  # Last timestep output\n",
        "        \n",
        "        # Fully connected layers\n",
        "        out = self.fc1(combined_hidden)\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "# Initialize model\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "\n",
        "model = LSTMSentimentClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "# Training parameters\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Initialize results tracking\n",
        "results = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"test_loss\": [],\n",
        "    \"test_acc\": []\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for sequences, labels in dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Evaluate\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(test_loss)\n",
        "    \n",
        "    # Store results\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Results\n",
        "plot_loss_curves(results)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on Sample Reviews\n",
        "def predict_sentiment(model, text, vocab, tokenizer, device):\n",
        "    \"\"\"Predict sentiment for a given text\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize and convert to indices\n",
        "    tokens = tokenizer(text.lower())\n",
        "    indices = [vocab[token] for token in tokens]\n",
        "    \n",
        "    # Convert to tensor and add batch dimension\n",
        "    sequence = torch.tensor([indices], dtype=torch.long).to(device)\n",
        "    \n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = model(sequence)\n",
        "        probabilities = torch.softmax(output, dim=1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "        confidence = probabilities[0][predicted_class].item()\n",
        "    \n",
        "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
        "    return sentiment, confidence\n",
        "\n",
        "# Test on sample reviews\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
        "    \"Terrible movie, waste of time. The acting was poor and the plot made no sense.\",\n",
        "    \"It was okay, nothing special but not bad either.\",\n",
        "    \"Amazing cinematography and great performances from all actors. Highly recommend!\",\n",
        "    \"Boring and predictable. I fell asleep halfway through.\"\n",
        "]\n",
        "\n",
        "print(\"Sample Predictions:\")\n",
        "print(\"=\" * 70)\n",
        "for review in test_reviews:\n",
        "    sentiment, confidence = predict_sentiment(model, review, vocab, tokenizer, device)\n",
        "    print(f\"Review: {review[:60]}...\")\n",
        "    print(f\"Prediction: {sentiment} (Confidence: {confidence:.2%})\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Model (Optional)\n",
        "model_path = Path(\"../03_Deep Learning Fundamentals/models\")\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'vocab': vocab,\n",
        "    'vocab_size': vocab_size,\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'hidden_dim': HIDDEN_DIM,\n",
        "    'num_layers': NUM_LAYERS,\n",
        "    'dropout': DROPOUT,\n",
        "}, model_path / \"lstm_imdb_sentiment.pth\")\n",
        "\n",
        "print(f\"Model saved to {model_path / 'lstm_imdb_sentiment.pth'}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
